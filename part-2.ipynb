{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9045008c32b5d644d863fe7f99e1a58",
     "grade": false,
     "grade_id": "cell-2db1ec9fd61f5d6b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project: Part 2 - Feature Extraction\n",
    "\n",
    "\n",
    "In any practical machine learning problem, the data preparation and feature extraction stages are the most important and time-consuming. The final project exposes you to a real-world dataset. In this part of the final project, you are responsible to creating features that will be meaningful for prediction. Features are evaluated based on Information Gain, which you implemented in [Assignment 2](../assignment-2/assignment-2.ipynb).\n",
    "\n",
    "Here is what will work well in this project:\n",
    "\n",
    "* Extract some sample data, load it in [R](https://www.r-project.org), and do some intial analysis. Feel free to build models there to get a feel for the best features.\n",
    "* Join the different tables--they are there for a reason. \n",
    "* Get creative.\n",
    "* Read some of the Kaggle competition forums and kernels. \n",
    "\n",
    "Here is what will NOT work:\n",
    "\n",
    "* Do not use only the features as provided in application_train.\n",
    "* Do not try implementing new learning algorithm in order to generate features. If you find something that works, investigate what features were helpful and add the features. \n",
    "* Do not build lookup tables \"embeddings\" or other things you might have read about but were not covered in class. \n",
    "* Do not try to build a kernel matrix on all pairs. Re-evaluate the kernel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ca0d151ba43bb4deb857d7f8c49f505",
     "grade": false,
     "grade_id": "cell-af1d85683fc29192",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79a9a43fb01aab13fbfdc805c3c72da2",
     "grade": false,
     "grade_id": "cell-99c6b85b81c0c166",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<SQLite3::Database:0x00000000036c1bc0 @tracefunc=nil, @authorizer=nil, @encoding=nil, @busy_handler=nil, @collations={}, @functions={}, @results_as_hash=true, @type_translation=nil, @readonly=true>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/home/dataset\"\n",
    "$dev_db = SQLite3::Database.new \"#{dir}/credit_risk_data_dev.db\", results_as_hash: true, readonly: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (10 Points)\n",
    "\n",
    "Implement ```create_dataset``` which runs an SQL query on a database and constructs a dataset like those we have used in this course. Add an ```id``` field for the ```SK_ID_CURR``` and store the ```TARGET``` in ```label```. \n",
    "\n",
    "If the query is:\n",
    "```sql\n",
    "select sk_id_curr, target, ext_source_1 from application_train  where ext_source_1 <> '' order by sk_id_curr;\n",
    "```\n",
    "\n",
    "then the result is:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"label\":1,\"id\":100002,\"features\":{\"ext_source_1\":0.08303696739132256}},\n",
    "    {\"label\":0,\"id\":100015,\"features\":{\"ext_source_1\":0.7220444501416448}}\n",
    "]\n",
    "...\n",
    "```\n",
    "Note the features should not include the ID or Label. Feature keys should be lowercase and only contain keys fo which ```key.is_a? String``` returns true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ad691ec475c4e6aa27160dbd7794be6",
     "grade": false,
     "grade_id": "cell-0a24a7c1d58b392f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":create_dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset db, sql\n",
    "  dataset = []\n",
    "  db.execute sql do |row|\n",
    "    # BEGIN YOUR CODE\n",
    "    r = Hash.new\n",
    "    r[\"features\"] = Hash.new\n",
    "    row.each do |key, value|\n",
    "      if key == \"SK_ID_CURR\"\n",
    "        r[\"id\"] = value\n",
    "      elsif key == \"TARGET\"\n",
    "        r[\"label\"] = value\n",
    "      else\n",
    "        if key.is_a? String\n",
    "          r[\"features\"][key.downcase] = value\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "    dataset << r\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  return dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc808c2af502b0b9896f6aa8f645f05d",
     "grade": true,
     "grade_id": "cell-8688ed35b3c26d22",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_11()\n",
    "  dataset = create_dataset $dev_db, \"select sk_id_curr, target, ext_source_1 from application_train where ext_source_1 <> '' \n",
    "order by sk_id_curr limit 37\"\n",
    "  assert_equal 37, dataset.size\n",
    "  assert_true(dataset[0][\"features\"].has_key? \"ext_source_1\")\n",
    "  assert_equal(1, dataset[0][\"features\"].size)\n",
    "  assert_equal(100002, dataset[0][\"id\"])  \n",
    "  assert_in_delta(0.08303696, dataset[0][\"features\"][\"ext_source_1\"], 1e-4)\n",
    "  assert_equal(1, dataset[0][\"label\"])    \n",
    "end\n",
    "\n",
    "test_11()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (20 points)\n",
    "\n",
    "Copy and revise **your** information gain calculation for numeric and categorical features, from [Assignment 2](../assignment-2/assignment-2.ipynb). Copy the following implementations\n",
    "\n",
    "* Class Distribution\n",
    "* Entropy\n",
    "* Information Gain after splitting\n",
    "* Information gain for numerical features (fast version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75bc54085b65d0a1164a7d8d030f55e6",
     "grade": false,
     "grade_id": "cell-b4fe1101959cb84a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def class_distribution dataset\n",
    "  # BEGIN YOUR CODE\n",
    "  group = dataset.group_by{|row| row[\"label\"]}\n",
    "  res = Hash.new {|h,k| h[k] = 0.0}\n",
    "  \n",
    "  group.each do |key, value|\n",
    "    res[key] = value.size.to_f / dataset.size.to_f\n",
    "  end\n",
    "  \n",
    "  return res\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4194853bca117dcb4696ac0ba1f37872",
     "grade": false,
     "grade_id": "cell-a956fb8dfa56395e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def entropy dist\n",
    "  # BEGIN YOUR CODE\n",
    "  t = dist.values.reduce(0.0, :+)\n",
    "  h = 0.0\n",
    "  dist.values.each do |d|\n",
    "    if d != 0\n",
    "      h -= (d / t) * Math.log(d / t)\n",
    "    end\n",
    "  end\n",
    "  return h\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b58a0a60ccd14c67c0e858cec1e2fe53",
     "grade": true,
     "grade_id": "cell-fe1d965fe70ab0a7",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_1()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  assert_in_delta(0.2686201883261589, h0, 1e-3)\n",
    "end\n",
    "\n",
    "test_12_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba92a5dee595dfa52d2ab80cb6ac9caa",
     "grade": false,
     "grade_id": "cell-9b94563fe65d0166",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def information_gain h0, splits\n",
    "  # BEGIN YOUR CODE\n",
    "  t = splits.map {|key, value| value.size}.reduce(0.0, :+)\n",
    "  information_gain = h0\n",
    "  splits.each do |key, value|\n",
    "    d = class_distribution(value)\n",
    "    e = entropy(d)\n",
    "    information_gain -= (value.size.to_f / t.to_f) * e.to_f\n",
    "  end\n",
    "  return information_gain\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7422895df5de3984725cbc2d5aa2a276",
     "grade": true,
     "grade_id": "cell-d88cded139a1edfd",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_2()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  \n",
    "  splits = dataset.group_by {|row| row[\"features\"][\"flag_own_car\"]}\n",
    "  ig = information_gain h0, splits\n",
    "  assert_in_delta(0.0002206258541794237, ig, 1e-4)\n",
    "end\n",
    "\n",
    "test_12_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "868e0a218c238e6c5c0e2a5fe4750a4c",
     "grade": false,
     "grade_id": "cell-05b14b2b932b0030",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_split_point_numeric x, h0, fname\n",
    "  # BEGIN YOUR CODE\n",
    "  sorted = x.sort_by {|row| row[\"features\"][fname] == nil ? 0 : row[\"features\"][fname]}\n",
    "  vl = Hash.new {|h,k| h[k] = 0.0}\n",
    "  vr = Hash.new {|h,k| h[k] = 0.0}\n",
    "  sorted.each do |row|\n",
    "    vr[row[\"label\"]] += 1.0\n",
    "  end\n",
    "  e_min = h0\n",
    "  t_max = sorted[0][\"features\"][fname]\n",
    "  i = 0\n",
    "  s = sorted.size\n",
    "  while i < s do\n",
    "    row = sorted[i]\n",
    "    vl[row[\"label\"]] += 1.0\n",
    "    vr[row[\"label\"]] -= 1.0\n",
    "    if i + 1 < s and row[\"features\"][fname] == sorted[i + 1][\"features\"][fname]\n",
    "      i += 1\n",
    "      next\n",
    "    end\n",
    "    e = ((i+1.0)/s)*entropy(vl)+((s-i-1.0)/s)*entropy(vr)\n",
    "    if e < e_min\n",
    "      e_min = e\n",
    "      t_max = sorted[i+1][\"features\"][fname]\n",
    "    end\n",
    "    i += 1\n",
    "  end\n",
    "  return [t_max, h0 - e_min]\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef61a807425ade33886ce44c836d1206",
     "grade": true,
     "grade_id": "cell-8d49eda92182a4b9",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_3()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  \n",
    "  t, ig = find_split_point_numeric dataset, h0, \"ext_source_1\"\n",
    "  assert_in_delta(0.009751743140812785, ig, 1e-4)\n",
    "end\n",
    "\n",
    "test_12_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (70 Points)\n",
    "\n",
    "Using whatever external software you want (hosted on your own devices), provide 15+ different features that have information >= 0.005. You may to implement several cells below, so please insert them above the test. \n",
    "\n",
    "Features must only be derived from the database but you are free to write whatever SQL queries you want. You may create temporary tables, but the database is read-only.\n",
    "\n",
    "Pay close attention to the following aspects of feature design:\n",
    "\n",
    "* Normalization: Z-score, L2, Min-Max, etc.\n",
    "* Sparsity / missing values\n",
    "* Frequency: Information is easily fooled by features with many values.\n",
    "* Joins: Some of the best features in this dataset combine two columns from different tables.\n",
    "* Transformations: One-hot, Binning, Discretization, Non-linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN YOUR CODE\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "  product = 0.0\n",
    "  x.each do |key, value|\n",
    "    if w[key] != nil\n",
    "      product += value * w[key]\n",
    "    end\n",
    "  end\n",
    "  return product\n",
    "  #END YOUR CODE\n",
    "end\n",
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot(w, w))\n",
    "  #END YOUR CODE\n",
    "end\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmean data\n",
    "  res = Hash.new\n",
    "  count = Hash.new\n",
    "  data.each do |sample|\n",
    "    sample[\"features\"].each do |key, value|\n",
    "      if res[key] == nil\n",
    "        res[key] = 0.0\n",
    "      end\n",
    "      if count[key] == nil\n",
    "          count[key] = 0\n",
    "      end\n",
    "      if value != nil and value.is_a?(String) == false\n",
    "        res[key] += value\n",
    "        count[key] += 1\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  res.each do |key, value|\n",
    "    res[key] = value / count[key]\n",
    "  end\n",
    "  return res\n",
    "end\n",
    "  \n",
    "def hstd data, mean\n",
    "  res = Hash.new\n",
    "  count = Hash.new\n",
    "  data.each do |sample|\n",
    "    sample[\"features\"].each do |key, value|\n",
    "      if res[key] == nil or res[key] == \"\" or res[key] == ''\n",
    "        res[key] = 0.0\n",
    "      end\n",
    "      if count[key] == nil\n",
    "        count[key] = 0\n",
    "      end\n",
    "      if value != nil and value.is_a?(String) == false\n",
    "        res[key] += (value - mean[key])**2\n",
    "        count[key] += 1\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  res.each do |key, value|\n",
    "    if count[key] > 1\n",
    "      res[key] = Math.sqrt(value / (count[key] - 1))\n",
    "    else\n",
    "      res[key] = Math.sqrt(value)\n",
    "    end\n",
    "  end\n",
    "  return res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize dataset\n",
    "  zdataset = dataset.clone\n",
    "  zdataset = dataset.collect do |r|\n",
    "    u = r.clone\n",
    "    u[\"features\"] = r[\"features\"].clone\n",
    "    u\n",
    "  end\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "  mean = hmean(zdataset)\n",
    "  std = hstd(zdataset, mean)\n",
    "  zdataset.each do |sample|\n",
    "    sample[\"features\"].each do |key, value|\n",
    "      if value.is_a?(String) == false\n",
    "        if std[key] > 0.0\n",
    "          sample[\"features\"][key] = (value - mean[key]) / std[key]\n",
    "        else\n",
    "          sample[\"features\"][key] = 0.0\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  #END YOUR CODE\n",
    "  return zdataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zero dataset\n",
    "  features = dataset.flat_map {|row| row[\"features\"].keys}.uniq\n",
    "  numeric_features = features.select {|k| dataset.reject {|row| row[\"features\"][k] == \"\"}.all? {|row| row[\"features\"].fetch(k, 0.0).is_a? Numeric}}\n",
    "  categorical_features = features.select {|k| dataset.all? {|row| row[\"features\"].fetch(k, \"\").is_a? String}}\n",
    "  dataset.each do |row|\n",
    "    numeric_features.each do |feature|\n",
    "      value = row[\"features\"][feature]\n",
    "      if value == \"\" or value == ''\n",
    "        row[\"features\"][feature] = 0.0\n",
    "      end\n",
    "    end\n",
    "    categorical_features.each do |feature|\n",
    "      value = row[\"features\"][feature]\n",
    "      if value == \"\" or value == ''\n",
    "        row[\"features\"][feature] = \"0\"\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mean dataset\n",
    "  features = dataset.flat_map {|row| row[\"features\"].keys}.uniq\n",
    "  numeric_features = features.select {|k| dataset.reject {|row| row[\"features\"][k] == \"\"}.all? {|row| row[\"features\"].fetch(k, 0.0).is_a? Numeric}}\n",
    "  categorical_features = features.select {|k| dataset.all? {|row| row[\"features\"].fetch(k, \"\").is_a? String}}\n",
    "  mean = hmean dataset \n",
    "  dataset.each do |row|\n",
    "    numeric_features.each do |feature|\n",
    "      value = row[\"features\"][feature]\n",
    "      if value == \"\" or value == ''\n",
    "        row[\"features\"][feature] = mean[feature]\n",
    "      end\n",
    "    end\n",
    "    categorical_features.each do |feature|\n",
    "      value = row[\"features\"][feature]\n",
    "      if value == \"\" or value == ''\n",
    "        row[\"features\"][feature] = \"0\"\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT target, sk_id_curr, ext_source_1, ext_source_2, ext_source_3 FROM application_train\"\n",
    "dataset = fill_mean dataset\n",
    "dataset.each do |row|\n",
    "  row[\"features\"][\"ext_prd\"] = row[\"features\"][\"ext_source_1\"] * row[\"features\"][\"ext_source_2\"] * row[\"features\"][\"ext_source_3\"]\n",
    "  row[\"features\"][\"ext_sum\"] = row[\"features\"][\"ext_source_1\"] + row[\"features\"][\"ext_source_2\"] + row[\"features\"][\"ext_source_3\"]\n",
    "  row[\"features\"][\"ext_avg\"] = row[\"features\"][\"ext_sum\"] / 3.0\n",
    "  row[\"features\"][\"ext_var\"] = (row[\"features\"][\"ext_source_1\"] - row[\"features\"][\"ext_avg\"])**2 +\n",
    "    (row[\"features\"][\"ext_source_2\"] - row[\"features\"][\"ext_avg\"])**2 + (row[\"features\"][\"ext_source_3\"] - row[\"features\"][\"ext_avg\"])**2\n",
    "  row[\"features\"][\"ext_std\"] = row[\"features\"][\"ext_var\"] / 2.0\n",
    "end\n",
    "puts dataset.size\n",
    "\n",
    "dist = class_distribution dataset\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_source_1\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_source_2\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_source_3\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_prd\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_avg\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_var\"\n",
    "puts ig\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"ext_std\"\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_doc = \"\"\n",
    "(2..21).each do |i|\n",
    "  flag_doc += \", flag_document_#{i.to_s}\"\n",
    "end\n",
    "\n",
    "\n",
    "dataset = create_dataset $dev_db, \"select target, sk_id_curr,\n",
    "flag_own_car, flag_own_realty, cnt_children, name_contract_type, code_gender, occupation_type\n",
    "name_type_suite, name_income_type, name_education_type, name_family_status, name_housing_type from application_train\"\n",
    "dataset.each do |row|\n",
    "  row[\"features\"][\"c1\"] = \n",
    "  row[\"features\"][\"flag_own_car\"].to_s + row[\"features\"][\"flag_own_realty\"].to_s + \n",
    "  row[\"features\"][\"name_income_type\"].to_s + row[\"features\"][\"name_housing_type\"].to_s\n",
    "  \n",
    "  row[\"features\"][\"c2\"] = \n",
    "  row[\"features\"][\"code_gender\"].to_s + row[\"features\"][\"cnt_children\"].to_s + \n",
    "  row[\"features\"][\"name_education_type\"].to_s + row[\"features\"][\"name_family_status\"].to_s\n",
    "  \n",
    "  row[\"features\"][\"c3\"] = \n",
    "  row[\"features\"][\"name_type_suite\"].to_s + row[\"features\"][\"name_income_type\"].to_s +\n",
    "  row[\"features\"][\"name_housing_type\"].to_s + row[\"features\"][\"name_contract_type\"].to_s\n",
    "  \n",
    "  row[\"features\"][\"c4\"] = \n",
    "  row[\"features\"][\"occupation_type\"].to_s + row[\"features\"][\"name_income_type\"].to_s + \n",
    "  row[\"features\"][\"name_education_type\"].to_s + row[\"features\"][\"name_housing_type\"].to_s\n",
    "  \n",
    "#   row[\"features\"][\"flag_doc_num\"] = row[\"features\"].map {|key, value| (key.include? \"flag\") ? value.to_i : 0}.reduce(0, :+).to_s\n",
    "end\n",
    "dist = class_distribution dataset\n",
    "h0 = entropy dist\n",
    "\n",
    "splits = dataset.group_by {|row| row[\"features\"][\"c1\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig\n",
    "\n",
    "splits = dataset.group_by {|row| row[\"features\"][\"c2\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig\n",
    "\n",
    "splits = dataset.group_by {|row| row[\"features\"][\"c3\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig\n",
    "\n",
    "splits = dataset.group_by {|row| row[\"features\"][\"c4\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig\n",
    "\n",
    "# splits = dataset.group_by {|row| row[\"features\"][\"flag_doc_num\"]}\n",
    "# ig = information_gain h0, splits\n",
    "# puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT target, sk_id_curr, region_population_relative, region_rating_client_w_city, REGION_RATING_CLIENT FROM application_train\"\n",
    "dataset = fill_mean dataset\n",
    "dataset.each do |row|\n",
    "  row[\"features\"][\"region_stat\"] = row[\"features\"][\"region_population_relative\"] - row[\"features\"][\"region_rating_client_w_city\"]\n",
    "end\n",
    "puts dataset.size\n",
    "dist = class_distribution dataset\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"region_stat\"\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_doc = \"\"\n",
    "(2..21).each do |i|\n",
    "  flag_doc += \", FLAG_DOCUMENT_#{i.to_s}\"\n",
    "end\n",
    "sql = \"SELECT target, sk_id_curr#{flag_doc.to_s} FROM application_train\"\n",
    "dataset = create_dataset $dev_db, sql.to_s\n",
    "\n",
    "dataset.each do |row|\n",
    "  row[\"features\"][\"flag_doc_sum\"] = row[\"features\"].map {|key, value| value}.reduce(0, :+)\n",
    "end\n",
    "dist = class_distribution dataset\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric dataset, h0, \"flag_doc_sum\"\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_doc = \"\"\n",
    "(2..21).each do |i|\n",
    "  flag_doc += \", flag_document_#{i.to_s}\"\n",
    "end\n",
    "\n",
    "amt_credit_bureau = \"AMT_REQ_CREDIT_BUREAU_HOUR, AMT_REQ_CREDIT_BUREAU_DAY, \n",
    "AMT_REQ_CREDIT_BUREAU_WEEK, AMT_REQ_CREDIT_BUREAU_MON, AMT_REQ_CREDIT_BUREAU_QRT, AMT_REQ_CREDIT_BUREAU_YEAR \"\n",
    "sql = \"SELECT target, sk_id_curr#{flag_doc.to_s}, #{amt_credit_bureau} FROM application_train\"\n",
    "dataset = create_dataset $dev_db, sql.to_s\n",
    "dataset = fill_mean dataset\n",
    "splitset = Array.new\n",
    "dataset.each do |row|\n",
    "  h = Hash.new\n",
    "  h[\"id\"] = row[\"id\"].clone\n",
    "  h[\"label\"] = row[\"label\"].clone\n",
    "  h[\"features\"] = Hash.new\n",
    "  h[\"features\"][\"flag_document_3\"] = row[\"features\"][\"flag_document_3\"]\n",
    "  h[\"features\"][\"flag_doc_sum\"] = row[\"features\"].map {|key, value| (key.include? \"flag\") ? value.to_i : 0}.reduce(0, :+).to_s\n",
    "#   puts h[\"features\"][\"flag_doc_sum\"]\n",
    "  h[\"features\"][\"amt_credit_bureau\"] = row[\"features\"].map {|key, value| (key.include? \"amt\") ? value : 0}.reduce(0, :+)\n",
    "  splitset << h\n",
    "end\n",
    "# splitset = z_normalize splitset\n",
    "# splitset.each do |row|\n",
    "#   row[\"features\"][\"flag_amt_combined\"] = row[\"features\"][\"flag_doc_sum\"] + row[\"features\"][\"amt_credit_bureau\"]\n",
    "# end\n",
    "\n",
    "dist = class_distribution splitset\n",
    "h0 = entropy dist\n",
    "\n",
    "splits = dataset.group_by {|row| row[\"features\"][\"amt_credit_bureau\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig\n",
    "\n",
    "# t1, ig1 = find_split_point_numeric splitset, h0, \"flag_document_3\"\n",
    "# puts ig1\n",
    "\n",
    "# t3, ig3 = find_split_point_numeric splitset, h0, \"flag_doc_sum\"\n",
    "# puts ig3\n",
    "\n",
    "# t2, ig2 = find_split_point_numeric splitset, h0, \"flag_doc_sum_weighted\"\n",
    "# puts ig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT application_train.sk_id_curr, \n",
    "application_train.target, application_train.DAYS_BIRTH, \n",
    "application_train.DAYS_EMPLOYED, application_train.DAYS_ID_PUBLISH,\n",
    "AVG(bureau.DAYS_CREDIT) AS days_credit FROM application_train LEFT JOIN bureau \n",
    "ON application_train.sk_id_curr=bureau.sk_id_curr \n",
    "GROUP BY application_train.sk_id_curr\"\n",
    "dataset = fill_mean dataset\n",
    "splitset = Array.new\n",
    "dataset.each do |row|\n",
    "  h = Hash.new\n",
    "  h[\"id\"] = row[\"id\"].clone\n",
    "  h[\"label\"] = row[\"label\"].clone\n",
    "  h[\"features\"] = Hash.new\n",
    "  h[\"features\"][\"days_birth_pow\"] = row[\"features\"][\"days_birth\"].to_f**2\n",
    "  h[\"features\"][\"days_credit\"] = row[\"features\"][\"days_credit\"].to_f\n",
    "  h[\"features\"][\"days_employed_pow\"] = row[\"features\"][\"days_employed\"].to_f**2\n",
    "  h[\"features\"][\"days_id_publish\"] = row[\"features\"][\"days_id_publish\"].to_f\n",
    "  \n",
    "  h[\"features\"][\"days_compound\"] = h[\"features\"][\"days_birth_pow\"] * h[\"features\"][\"days_employed_pow\"] * \n",
    "  h[\"features\"][\"days_credit\"] * h[\"features\"][\"days_id_publish\"]\n",
    "  splitset << h\n",
    "end\n",
    "dist = class_distribution splitset\n",
    "\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric splitset, h0, \"days_compound\"\n",
    "puts splitset.size\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT application_train.sk_id_curr, application_train.target, \n",
    "application_train.amt_goods_price, application_train.amt_income_total, \n",
    "application_train.amt_credit, application_train.AMT_ANNUITY AS at_amt_annuity, \n",
    "AVG(previous_application.NAME_CONTRACT_TYPE) AS prev_name_contract_type, \n",
    "AVG(previous_application.AMT_APPLICATION) AS prev_amt_application, \n",
    "AVG(previous_application.AMT_DOWN_PAYMENT) AS prev_amt_down_payment,\n",
    "AVG(previous_application.CODE_REJECT_REASON) AS prev_code_reject_reason,\n",
    "AVG(previous_application.RATE_INTEREST_PRIMARY) AS rate_interest_primary, \n",
    "AVG(previous_application.RATE_INTEREST_PRIVILEGED) AS rate_interest_privileged\n",
    "FROM application_train LEFT JOIN previous_application \n",
    "ON application_train.sk_id_curr=previous_application.sk_id_curr \n",
    "GROUP BY application_train.sk_id_curr\"\n",
    "dataset = fill_mean dataset\n",
    "splitset = Array.new\n",
    "dataset.each do |row|\n",
    "  h = Hash.new\n",
    "  h[\"id\"] = row[\"id\"].clone\n",
    "  h[\"label\"] = row[\"label\"].clone\n",
    "  h[\"features\"] = Hash.new\n",
    "  h[\"features\"][\"rate_interest_primary\"] = row[\"features\"][\"rate_interest_primary\"]\n",
    "  h[\"features\"][\"rate_interest_privileged\"] = row[\"features\"][\"rate_interest_privileged\"]\n",
    "  h[\"features\"][\"prev_name_contract_type\"] = row[\"features\"][\"prev_name_contract_type\"]\n",
    "  h[\"features\"][\"prev_amt_application\"] = row[\"features\"][\"prev_amt_application\"]\n",
    "  h[\"features\"][\"prev_amt_down_payment\"] = row[\"features\"][\"prev_amt_down_payment\"]\n",
    "  h[\"features\"][\"prev_code_reject_reason\"] = row[\"features\"][\"prev_code_reject_reason\"]\n",
    "  \n",
    "  h[\"features\"][\"compound\"] = h[\"features\"][\"rate_interest_primary\"]**2\n",
    "  splitset << h\n",
    "end\n",
    "\n",
    "dist = class_distribution splitset\n",
    "\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric splitset, h0, \"compound\"\n",
    "puts splitset.size\n",
    "puts ig\n",
    "\n",
    "splits = splitset.group_by {|row| row[\"features\"][\"prev_amt_application\"]}\n",
    "ig = information_gain h0, splits\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT application_train.sk_id_curr, application_train.target, \n",
    "application_train.AMT_ANNUITY, application_train.AMT_GOODS_PRICE, application_train.AMT_CREDIT, application_train.amt_income_total, \n",
    "AVG(previous_application.AMT_ANNUITY) AS prev_amt_annuity, AVG(previous_application.AMT_CREDIT) AS prev_amt_credit, \n",
    "AVG(previous_application.AMT_GOODS_PRICE) AS prev_amt_goods_price, AVG(previous_application.RATE_INTEREST_PRIMARY), \n",
    "AVG(previous_application.RATE_INTEREST_PRIVILEGED),\n",
    "AVG(bureau.AMT_ANNUITY) AS bu_amt_annuity, AVG(bureau.AMT_CREDIT_SUM) AS bu_amt_credit\n",
    "FROM application_train LEFT JOIN previous_application \n",
    "ON application_train.sk_id_curr=previous_application.sk_id_curr \n",
    "LEFT JOIN bureau ON application_train.sk_id_curr=bureau.sk_id_curr\n",
    "GROUP BY application_train.sk_id_curr\"\n",
    "dataset = fill_mean dataset\n",
    "splitset = Array.new\n",
    "dataset.each do |row|\n",
    "  h = Hash.new\n",
    "  h[\"id\"] = row[\"id\"].clone\n",
    "  h[\"label\"] = row[\"label\"].clone\n",
    "  h[\"features\"] = Hash.new\n",
    "  h[\"features\"][\"prev_amt_annuity\"] = row[\"features\"][\"prev_amt_annuity\"]\n",
    "  h[\"features\"][\"prev_amt_credit\"] = row[\"features\"][\"prev_amt_credit\"]\n",
    "  h[\"features\"][\"prev_amt_goods_price\"] = row[\"features\"][\"prev_amt_goods_price\"]\n",
    "  \n",
    "  h[\"features\"][\"amt_annuity\"] = row[\"features\"][\"amt_annuity\"]\n",
    "  h[\"features\"][\"amt_credit\"] = row[\"features\"][\"amt_credit\"]\n",
    "  h[\"features\"][\"amt_goods_price\"] = row[\"features\"][\"amt_goods_price\"]\n",
    "  \n",
    "  h[\"features\"][\"bu_amt_annuity\"] = row[\"features\"][\"bu_amt_annuity\"]\n",
    "  h[\"features\"][\"bu_amt_credit\"] = row[\"features\"][\"bu_amt_credit\"]\n",
    "  if h[\"features\"][\"prev_amt_annuity\"] == 0.0\n",
    "    h[\"features\"][\"prev_credit_to_annuity\"] = h[\"features\"][\"prev_amt_credit\"]\n",
    "  else\n",
    "    h[\"features\"][\"prev_credit_to_annuity\"] = h[\"features\"][\"prev_amt_credit\"] / h[\"features\"][\"prev_amt_annuity\"]\n",
    "  end\n",
    "  if h[\"features\"][\"bu_amt_annuity\"] == 0.0\n",
    "    h[\"features\"][\"bu_credit_to_annuity\"] = h[\"features\"][\"bu_amt_credit\"]\n",
    "  else\n",
    "    h[\"features\"][\"bu_credit_to_annuity\"] = h[\"features\"][\"bu_amt_credit\"] / h[\"features\"][\"bu_amt_annuity\"]\n",
    "  end\n",
    "  if h[\"features\"][\"amt_annuity\"] == 0.0\n",
    "    h[\"features\"][\"credit_to_annuity\"] = h[\"features\"][\"amt_credit\"]\n",
    "  else\n",
    "    h[\"features\"][\"credit_to_annuity\"] = h[\"features\"][\"amt_credit\"] / h[\"features\"][\"amt_annuity\"]\n",
    "  end\n",
    "#   h[\"features\"][\"credit_sum\"] = h[\"features\"][\"amt_credit\"] + h[\"features\"][\"prev_amt_credit\"] + h[\"features\"][\"bu_amt_credit\"]\n",
    "#   h[\"features\"][\"annuity_sum\"] = h[\"features\"][\"amt_annuity\"] + h[\"features\"][\"prev_amt_annuity\"] + h[\"features\"][\"bu_amt_annuity\"]\n",
    "#   h[\"features\"][\"goods_price_sum\"] = h[\"features\"][\"amt_goods_price\"] + h[\"features\"][\"prev_amt_goods_price\"]\n",
    "\n",
    "  h[\"features\"][\"credit_minus_price\"] = h[\"features\"][\"amt_credit\"] - h[\"features\"][\"amt_goods_price\"]\n",
    "  h[\"features\"][\"prev_credit_minus_price\"] = h[\"features\"][\"prev_amt_credit\"] - h[\"features\"][\"prev_amt_goods_price\"]\n",
    "  h[\"features\"][\"credit_minus_price_compound\"] = \n",
    "  (h[\"features\"][\"credit_minus_price\"] + h[\"features\"][\"prev_credit_minus_price\"])\n",
    "  \n",
    "  h[\"features\"][\"compound_credit_to_annuity\"] =\n",
    "  Math.log10(h[\"features\"][\"bu_credit_to_annuity\"]**4 * h[\"features\"][\"credit_to_annuity\"])\n",
    "  \n",
    "  splitset << h\n",
    "end\n",
    "\n",
    "dist = class_distribution splitset\n",
    "\n",
    "h0 = entropy dist\n",
    "\n",
    "t, ig = find_split_point_numeric splitset, h0, \"credit_minus_price_compound\"\n",
    "puts splitset.size\n",
    "puts ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset $dev_db, \"SELECT TARGET, SK_ID_CURR AS COUNT FROM application_train WHERE SK_ID_CURR IN (SELECT SK_ID_CURR FROM previous_application GROUP BY SK_ID_CURR) GROUP BY SK_ID_CURR\"\n",
    "dataset.each do |sample|\n",
    "  puts sample\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add extra cells as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "60cdc4cc43071b459fbe306658451e3e",
     "grade": false,
     "grade_id": "cell-81b7747d7b0f0592",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features db\n",
    "  dataset = create_dataset db, \"SELECT\n",
    "  application_train.sk_id_curr,\n",
    "  application_train.target,\n",
    "  application_train.ext_source_1,\n",
    "  application_train.ext_source_2,\n",
    "  application_train.ext_source_3,\n",
    "  application_train.flag_own_car,\n",
    "  application_train.flag_own_realty,\n",
    "  application_train.cnt_children,\n",
    "  application_train.name_contract_type,\n",
    "  application_train.code_gender,\n",
    "  application_train.name_type_suite,\n",
    "  application_train.name_income_type,\n",
    "  application_train.name_education_type,\n",
    "  application_train.name_family_status,\n",
    "  application_train.name_housing_type,\n",
    "  application_train.occupation_type,\n",
    "  application_train.AMT_ANNUITY,\n",
    "  application_train.AMT_GOODS_PRICE,\n",
    "  application_train.AMT_CREDIT,\n",
    "  application_train.DAYS_BIRTH,\n",
    "  application_train.DAYS_EMPLOYED,\n",
    "  application_train.DAYS_ID_PUBLISH,\n",
    "  AVG(previous_application.AMT_ANNUITY) AS prev_amt_annuity,\n",
    "  AVG(previous_application.AMT_CREDIT) AS prev_amt_credit,\n",
    "  AVG(previous_application.AMT_GOODS_PRICE) AS prev_amt_goods_price,\n",
    "  AVG(bureau.AMT_ANNUITY) AS bu_amt_annuity,\n",
    "  AVG(bureau.AMT_CREDIT_SUM_DEBT) AS amt_credit_sum_debt,\n",
    "  AVG(bureau.DAYS_CREDIT) AS days_credit,\n",
    "  AVG(bureau.AMT_CREDIT_SUM) AS amt_credit_sum,\n",
    "  AVG(installments_payments.AMT_PAYMENT) AS amt_payment,\n",
    "  AVG(installments_payments.DAYS_INSTALMENT) AS days_instalment\n",
    "  FROM\n",
    "  application_train\n",
    "  LEFT JOIN bureau ON application_train.sk_id_curr = bureau.sk_id_curr\n",
    "  LEFT JOIN previous_application ON application_train.sk_id_curr = previous_application.sk_id_curr\n",
    "  LEFT JOIN installments_payments ON previous_application.sk_id_prev = installments_payments.sk_id_prev\n",
    "  GROUP BY\n",
    "  application_train.sk_id_curr\"\n",
    "  dataset = fill_mean dataset\n",
    "  splitset = Array.new\n",
    "  \n",
    "#   credit_to_annuity_max = dataset.map {|row| row[\"features\"][\"bu_amt_annuity\"].to_f == 0.0 ? row[\"features\"][\"amt_credit_sum\"].to_f : (row[\"features\"][\"amt_credit_sum\"].to_f / row[\"features\"][\"bu_amt_annuity\"].to_f)}.max\n",
    "#   credit_to_annuity_min = dataset.map {|row| row[\"features\"][\"bu_amt_annuity\"].to_f == 0.0 ? row[\"features\"][\"amt_credit_sum\"].to_f : (row[\"features\"][\"amt_credit_sum\"].to_f / row[\"features\"][\"bu_amt_annuity\"].to_f)}.min\n",
    "    \n",
    "  \n",
    "  dataset.each do |row|\n",
    "    h = Hash.new\n",
    "    h[\"features\"] = Hash.new\n",
    "    h[\"id\"] = row[\"id\"].clone\n",
    "    h[\"label\"] = row[\"label\"].clone\n",
    "    \n",
    "    h[\"features\"][\"ext_source_2\"] = row[\"features\"][\"ext_source_2\"]\n",
    "    h[\"features\"][\"ext_source_3\"] = row[\"features\"][\"ext_source_3\"]\n",
    "    h[\"features\"][\"ext_prd\"] = \n",
    "    row[\"features\"][\"ext_source_1\"] * row[\"features\"][\"ext_source_2\"] * row[\"features\"][\"ext_source_3\"]\n",
    "    h[\"features\"][\"ext_avg\"] = \n",
    "    (row[\"features\"][\"ext_source_1\"] + row[\"features\"][\"ext_source_2\"] + row[\"features\"][\"ext_source_3\"]) / 3.0\n",
    "    \n",
    "    h[\"features\"][\"c_1\"] = \n",
    "    row[\"features\"][\"flag_own_car\"].to_s + row[\"features\"][\"flag_own_realty\"].to_s + \n",
    "    row[\"features\"][\"name_income_type\"].to_s + row[\"features\"][\"name_housing_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"c_2\"] = \n",
    "    row[\"features\"][\"code_gender\"].to_s + row[\"features\"][\"cnt_children\"].to_s + \n",
    "    row[\"features\"][\"name_education_type\"].to_s + row[\"features\"][\"name_family_status\"].to_s\n",
    "\n",
    "    h[\"features\"][\"c_3\"] = \n",
    "    row[\"features\"][\"name_type_suite\"].to_s + row[\"features\"][\"name_income_type\"].to_s +\n",
    "    row[\"features\"][\"name_housing_type\"].to_s + row[\"features\"][\"name_contract_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"c_4\"] = \n",
    "    row[\"features\"][\"occupation_type\"].to_s + row[\"features\"][\"name_education_type\"].to_s + \n",
    "    row[\"features\"][\"name_housing_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"days_compound\"] = \n",
    "    (row[\"features\"][\"days_birth\"].to_f)**2 * (row[\"features\"][\"days_employed\"].to_f)**2 * \n",
    "    (row[\"features\"][\"days_credit\"].to_f * row[\"features\"][\"days_id_publish\"].to_f) / 10**15\n",
    "    \n",
    "    h[\"features\"][\"days_birth_ext\"] = \n",
    "    (row[\"features\"][\"days_birth\"].to_f)**2 * \n",
    "    row[\"features\"][\"ext_source_2\"].to_f / 10**5\n",
    "    \n",
    "    h[\"features\"][\"days_credit_ext\"] = \n",
    "    (row[\"features\"][\"days_credit\"].to_f)**2 * \n",
    "    row[\"features\"][\"ext_source_2\"].to_f / 10**5\n",
    "    \n",
    "    amt_annuity = row[\"features\"][\"amt_annuity\"].to_f\n",
    "    amt_credit = row[\"features\"][\"amt_credit\"].to_f\n",
    "    bu_amt_annuity = row[\"features\"][\"bu_amt_annuity\"].to_f\n",
    "    bu_amt_credit = row[\"features\"][\"amt_credit_sum\"].to_f\n",
    "    amt_credit_sum_debt = row[\"features\"][\"amt_credit_sum_debt\"].to_f\n",
    "    amt_credit_sum = row[\"features\"][\"amt_credit_sum\"].to_f\n",
    "    amt_goods_price = row[\"features\"][\"amt_goods_price\"].to_f\n",
    "    prev_amt_credit = row[\"features\"][\"prev_amt_credit\"].to_f\n",
    "    prev_amt_goods_price = row[\"features\"][\"prev_amt_goods_price\"].to_f\n",
    "    amt_payment = row[\"features\"][\"amt_payment\"].to_f\n",
    "    days_instalment = row[\"features\"][\"days_instalment\"].to_f\n",
    "    \n",
    "    credit_to_annuity = 0.0\n",
    "    if amt_annuity == 0.0\n",
    "      credit_to_annuity = amt_credit\n",
    "    else\n",
    "      credit_to_annuity = amt_credit / amt_annuity\n",
    "    end\n",
    "    \n",
    "    bu_credit_to_annuity = 0.0\n",
    "    if bu_amt_annuity == 0.0\n",
    "      bu_credit_to_annuity = bu_amt_credit\n",
    "    else\n",
    "      bu_credit_to_annuity = bu_amt_credit / bu_amt_annuity\n",
    "    end\n",
    "    \n",
    "    if amt_credit_sum == 0.0\n",
    "      h[\"features\"][\"debt_to_credit\"] = (amt_credit_sum_debt**3) + h[\"features\"][\"ext_source_2\"]\n",
    "    else\n",
    "      h[\"features\"][\"debt_to_credit\"] = \n",
    "      ((amt_credit_sum_debt / amt_credit_sum)**3) + h[\"features\"][\"ext_source_2\"]\n",
    "    end\n",
    "    \n",
    "    credit_minus_price = amt_credit - amt_goods_price\n",
    "    prev_credit_minus_price = prev_amt_credit - prev_amt_goods_price\n",
    "    h[\"features\"][\"credit_minus_price_ext\"] = \n",
    "    (((credit_minus_price + prev_credit_minus_price)) / row[\"features\"][\"ext_source_2\"])\n",
    "    \n",
    "    payment_time = (amt_payment * days_instalment)\n",
    "    h[\"features\"][\"payment_time_ext\"] = \n",
    "    (-payment_time * h[\"features\"][\"ext_source_2\"])\n",
    "    \n",
    "    h[\"features\"][\"credit_to_annuity_ext\"] = \n",
    "    (credit_to_annuity * bu_credit_to_annuity * h[\"features\"][\"ext_source_3\"]**3)\n",
    "\n",
    "    splitset << h\n",
    "  end\n",
    "  splitset = z_normalize splitset \n",
    "  return splitset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = extract_features($dev_db)\n",
    "dist = class_distribution dataset\n",
    "h0 = entropy dist\n",
    "features = dataset.flat_map {|row| row[\"features\"].keys}.uniq\n",
    "puts dataset.size\n",
    "numeric_features = features.select {|k| dataset.reject {|row| row[\"features\"][k] == \"\"}.all? {|row| row[\"features\"].fetch(k, 0.0).is_a? Numeric}}\n",
    "categorical_features = features.select {|k| dataset.all? {|row| row[\"features\"].fetch(k, \"\").is_a? String}}\n",
    "numeric_features.each do |feature|\n",
    "  t, ig = find_split_point_numeric dataset, h0, feature\n",
    "  puts feature\n",
    "  puts ig\n",
    "  puts \"-----------------\"\n",
    "end\n",
    "categorical_features.each do |feature|\n",
    "  splits = dataset.group_by {|row| row[\"features\"][feature]}\n",
    "  ig = information_gain h0, splits\n",
    "  puts splits.size\n",
    "  puts feature\n",
    "  puts ig\n",
    "  puts \"-----------------\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aa9953ab7fd9e04eaed43e1dac0e393",
     "grade": false,
     "grade_id": "cell-8c402c9f742d0779",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "extracted_dataset = extract_features($dev_db)\n",
    "extracted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4597536fdbf10caf3336be5abe46f2c",
     "grade": true,
     "grade_id": "cell-ef86efb741a938de",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_not_nil extracted_dataset\n",
    "assert_equal 15334, extracted_dataset.size\n",
    "assert_true(extracted_dataset.all? {|row| row[\"features\"].size >= 8}, \"At least 6 non-zero features per row\")\n",
    "assert_true(extracted_dataset.flat_map {|row| row[\"features\"].keys}.uniq.size >= 15,  \"At least 15 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe8a6cc82645651810e2e301d81d06c5",
     "grade": true,
     "grade_id": "cell-23647d16738e3205",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal 15334, extracted_dataset.collect {|row| row[\"id\"]}.uniq.size\n",
    "assert_equal 2, extracted_dataset.collect {|row| row[\"label\"]}.uniq.size\n",
    "\n",
    "h0 = entropy(class_distribution(extracted_dataset))\n",
    "assert_in_delta(0.2797684909805576, h0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "773639e034e9c13094d055ea55cc4911",
     "grade": true,
     "grade_id": "cell-fe9232a9bed335ff",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "features = extracted_dataset.flat_map {|row| row[\"features\"].keys}.uniq\n",
    "numeric_features = features.select {|k| extracted_dataset.reject {|row| row[\"features\"][k] == \"\"}.all? {|row| row[\"features\"].fetch(k, 0.0).is_a? Numeric}}\n",
    "\n",
    "assert_true(numeric_features.size >= 4, \"At least 4 numeric features\")\n",
    "def test_ig_numeric extracted_dataset, h0, test_feature1\n",
    "  t, ig = find_split_point_numeric extracted_dataset, h0, test_feature1\n",
    "  assert_true(ig >= 0.005, \"Expected information gain for '#{test_feature1}' > 0.005\")\n",
    "  return test_feature1\n",
    "end\n",
    "\n",
    "test_ig_numeric extracted_dataset, h0, numeric_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0cefa2dba0e524d1aaaf262dcf46eeb5",
     "grade": true,
     "grade_id": "cell-e3cfb6234aae3cd6",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_ig_numeric extracted_dataset, h0, numeric_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43b12cb9172f05d3cfb69345edf5caec",
     "grade": true,
     "grade_id": "cell-3af2ae9935c80d63",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_ig_numeric extracted_dataset, h0, numeric_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02a54a39ba9c41be717488e7b0451902",
     "grade": true,
     "grade_id": "cell-7eb5e45fb12bab22",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "3.upto(numeric_features.size - 1) do |i|\n",
    "  test_ig_numeric extracted_dataset, h0, numeric_features[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9401c8458fc1db657539eef316489f6",
     "grade": true,
     "grade_id": "cell-433d61446fb448fb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = features.select {|k| extracted_dataset.all? {|row| row[\"features\"].fetch(k, \"\").is_a? String}}\n",
    "\n",
    "assert_true(categorical_features.size >= 4, \"At least 4 categorical features\")\n",
    "\n",
    "def test_ig_categorical extracted_dataset, h0, test_feature1\n",
    "  splits = extracted_dataset.group_by {|row| row[\"features\"][test_feature1]}\n",
    "  ig = information_gain h0, splits\n",
    "  puts ig\n",
    "  assert_true(ig >= 0.005, \"Expected information gain for '#{test_feature1}' > 0.005\")\n",
    "  return test_feature1\n",
    "end\n",
    "\n",
    "test_ig_categorical extracted_dataset, h0, categorical_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecc1f4e97d6c2a0809f21488f2574195",
     "grade": true,
     "grade_id": "cell-c066b4ac7af283e4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_ig_categorical extracted_dataset, h0, categorical_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc543b59d6c6c0b4e70e6ea5ae4d1d37",
     "grade": true,
     "grade_id": "cell-cace39d2ef8a94bc",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_ig_categorical extracted_dataset, h0, categorical_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af050a047b9388cfbe82a35515d10b27",
     "grade": true,
     "grade_id": "cell-c57fca43e0641c05",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "3.upto(categorical_features.size - 1) do |i|\n",
    "  test_ig_categorical extracted_dataset, h0, categorical_features[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
