{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c336275b386d6439ea67e77e96d9045",
     "grade": false,
     "grade_id": "cell-d21a86d292a3d7ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project Report\n",
    "\n",
    "Write your final project report in the cells below. All written material must be completed prior to your presentation slot. Including figures, appendices, etc. a printed version of this document should be no more than 10 pages and no less than 6 pages. \n",
    "\n",
    "The following cell is used for overall feedback and deductions for length, content, and style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ce7ea5533b1effff2e13157180854ce3",
     "grade": true,
     "grade_id": "cell-343bed4feca7930f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "qu.zh@husky.neu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "_(Why would people want to study this dataset and what is the primary task. Find out what the \"target\" variable means and why the customer is interested in running a competition on this dataset.) 2-3 paragraphs_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1b7d89f8e38774cd0b2d59d09f04a82b",
     "grade": true,
     "grade_id": "cell-c9bb94ed861b91bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n",
    "\n",
    "Home Credit is a financial group that strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
    "\n",
    "While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propsed method\n",
    "_(Present an 1-paragraph description of your method and why you believe it is better that the other things you have tried)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ad319c78971c6e4155af9a1e0aa2cf64",
     "grade": true,
     "grade_id": "cell-6085b28e8af4d158",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The model that I chose is Stochastic Gradient Descent with Logistic Regression with L2 regularization. I have implemented decision tree, Stochastic Gradient Descent with Linear regression with L2 regularization, Stochastic Gradient Descent with Hinge Loss Classifier. Considering the size of the dataset is nearly 300K, decision tree with 10-20 features would require an extremely long time to properly train and tune. Neural Network faces the same problem as the decision tree model. Among all other models that I have implemented and tested thoroughly with hyperparameter tuning, Logistic Regression have shown robust suprior performance comparing to all other choices. The training time required for Logistic regression to converge given the provided dataset is also very reasonable, so I finally decided to use Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "_(Find (5-6) examples of people who have worked on similar dataset from the literature. Note: Literature == Published paper in a conference (not stack overflow). Briefly describe in 1-2 sentences the kinds of features, algorithms, or other methods they applied. Also explain why you believe your method is better. Provide a numbered reference id that will appear later in the references section. 3-5 paragraphs_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede8e26b7174b9d514f4cf789af54bde",
     "grade": true,
     "grade_id": "cell-299d7142a9907f1d",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[A machine learning approach for predicting bank credit worthiness](https://ieeexplore.ieee.org/document/7585216/references#references)\n",
    "\n",
    "This study used some simple features like series of payment amount, bill amount, and the age, sex, marrige status of the applicant to predict the ability of the client to pay their loan in the next month. They used several traditional algorithms and ensemble learning algorithms. Logistic regression achieved the reasonably good accuracy among all the traditional models. Despite the fact that ensemble learning have better accuracy, considering the implementation difficulty and complexity of the algorithm, Logistic regression is still a very good choice.\n",
    "\n",
    "[Credit rating by hybrid machine learning techniques](https://www.sciencedirect.com/science/article/pii/S1568494609001215)\n",
    "\n",
    "This study used some real bank information of clients from Taiwan as dataset and trying to predict whether the client has a \"good\" or \"bad\" credit. The model that they implemented is a hybrid of classification and clustering algorithm, the hybrid of logistic regression with a base line classification model achieved the highest accuracy comparing to decision tree, neural network, and naive bayes hybrid models. Which further illustrate using logistic regression to predict credit with optimal features is still one of the best choice.\n",
    "\n",
    "[A data-driven approach to predict the success of bank telemarketing](https://www.sciencedirect.com/science/article/pii/S016792361400061X)\n",
    "\n",
    "This study utilized data mining technique to predict the success of telemarketing calls for selling bank long-term deposits. The data is from a Portuguese retail bank. Again, Logistic regression beats decision tree and SVM and achieved the highest accuracy.\n",
    "\n",
    "[Empirical research of hybridizing principal component analysis with multivariate discriminant analysis and logistic regression for business failure prediction](https://www.sciencedirect.com/science/article/pii/S0957417410012819)\n",
    "\n",
    "This study used logistic regression with different selected feature set to compare the accuracy of predicting business failures. Logistic regression have shown very stable accuracy level even with different features selection algorithms.\n",
    "\n",
    "[Machine Learning in Banking Risk Management: A Literature Review](https://www.mdpi.com/2227-9091/7/1/29/htm)\n",
    "\n",
    "This is a study discussing the accuracy of different machine learning algorithms predicting different type of risks related to bank. Logistic regression was specificly mentioned in this paper as one of the best traditional model to predict credit risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related implementations\n",
    "_(Find (2-3) examples of what people in Kaggle have done on this particular dataset [[2]](https://www.kaggle.com). Reference the URL of their kernel, post, etc. Describe in 1-2 sentences what they have done and why you think your method is better.) 2-3 paragraphs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b53e3ee2a6b1cb541dd35a70f821025",
     "grade": true,
     "grade_id": "cell-a30ca527d1596437",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[LighGBM_with_Selected_Features](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)\n",
    "\n",
    "This competitor used a lot of single features and some selected combined featured across different tables and LightGBM algorithm to train the model and achieved 0.796 accuracy level. The size of the features set is not realistic given the server that we have. The contribution of a lot of features are trivial. The number of features that I used are much more condense and significant to the model.\n",
    "\n",
    "[LightGBM with Simple Features](https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features)\n",
    "\n",
    "This competitor used similiar approach to the first one. However, the feature set is much smaller. Just like most of the top competitors, the algorithm used is still LightBGM. Given the exisiting algorithms that we have, I still think logistic regression performs reasonably good. The feature set that I implemented is very similiar but even smaller for better performance of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "_(Data Analysis: Describe the data analysis you have completed, include 1-2 plots of the most useful features or learnings you have obtained from the dataset. Do not include the code, but do include formulas to anything you have calculated such as different feature combinations, feature selection, or analysis methods. You must use at least one clustering algorithm we have seen in class for an analysis of the data. Provide a link to the specific notebook cell in previous notebooks as a reference.)_ 5-6 paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dadf3d2d4ff41cb2e933d964f940c8b4",
     "grade": true,
     "grade_id": "cell-f6715c211519644e",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Feature Engineering:\n",
    "\n",
    "I started off by dicovering the importance of all the single features across different tables, the following plot is from a Kaggle discussion forum post. The author did a thorough comparison of single features using random forest.\n",
    "\n",
    "<img src=\"single_feature_importance.png\">\n",
    "\n",
    "Then I did K-mean clustering on some of the most important features discovered on previous plot. As expected, the distribution of those features, more specifically, EXT_SOURCE_1 and EXT_SOURCE_2 of different target labels are significantly different from each other. Which further convinced me that those features would have dramatic impact on our model prediction.\n",
    "\n",
    "The next step was trying to combine different features across the table and discovering new features. I have looked up tons of discussion forums and tested tons of combinations using information gain. Some of the features turns out to be very useful for my model. \n",
    "\n",
    "<img src=\"combined_features_information_gain.png\">\n",
    "\n",
    "\n",
    "However, the problem with my analysis in part 2 was that a lot of combined features used EXT_SOURCE_2 as one of the option which did give a boost in information gain yet creates extremely high internal correlation of the feature set. In the final feature set I removed EXT_SOURCE_2 from those combination to improve accuracy.\n",
    "\n",
    "The final feature set that I used is more robust to internal correlation problem, I have also incorporated more numerical features that gives small boosts to the model accuracy. I stopped adding more features when I figured the training time was already up to my desired time and adding more features would not have a significant impact to the model.\n",
    "\n",
    "Here is the final feature set and its corresponding information gains\n",
    "\n",
    "### ext_source_1\n",
    "0.003045974148855435\n",
    "\n",
    "### ext_source_2\n",
    "0.009073896799364323\n",
    "\n",
    "\n",
    "### ext_source_3\n",
    "0.006200018047579359\n",
    "\n",
    "\n",
    "### ext_prd\n",
    "0.01451460739198579\n",
    "\n",
    "\n",
    "### ext_sum\n",
    "0.015013068877422542\n",
    "\n",
    "\n",
    "### ext_source_std\n",
    "0.0037505345715957428\n",
    "\n",
    "\n",
    "### days_birth\n",
    "0.002382000832049258\n",
    "\n",
    "\n",
    "### days_credit\n",
    "0.0038779039529066583\n",
    "\n",
    "\n",
    "### days_employed\n",
    "0.0020948280555375676\n",
    "\n",
    "\n",
    "### days_id_publish\n",
    "0.0010614961427801162\n",
    "\n",
    "\n",
    "### days_registration\n",
    "0.0007879898688690212\n",
    "\n",
    "\n",
    "### own_car_age\n",
    "0.0007292871017848257\n",
    "\n",
    "\n",
    "### days_last_phone_change\n",
    "0.0015254454633181758\n",
    "\n",
    "\n",
    "### credit_to_birth\n",
    "0.0026406389004863806\n",
    "\n",
    "\n",
    "### employed_to_birth\n",
    "0.0013357239869863369\n",
    "\n",
    "\n",
    "### car_age_to_birth\n",
    "0.00301540829002811\n",
    "\n",
    "\n",
    "### cnt_children\n",
    "0.00022995334538189471\n",
    "\n",
    "\n",
    "### cnt_fam_members\n",
    "0.0001644055939510336\n",
    "\n",
    "### flag_document_3\n",
    "0.0008386068846497619\n",
    "\n",
    "\n",
    "### credit_minus_price\n",
    "0.001386961618309701\n",
    "\n",
    "\n",
    "### prev_credit_minus_price\n",
    "0.0008103255518315255\n",
    "\n",
    "\n",
    "### payment_instalment_prd\n",
    "0.0014767582269274993\n",
    "\n",
    "\n",
    "### payment_to_annuity\n",
    "0.001162146962128574\n",
    "\n",
    "\n",
    "### credit_to_annuity\n",
    "0.001114999108280168\n",
    "\n",
    "\n",
    "### credit_to_income\n",
    "0.0005082765098558539\n",
    "\n",
    "\n",
    "### annuity_to_income\n",
    "0.0009028588063333398\n",
    "\n",
    "\n",
    "### bu_credit_to_annuity\n",
    "0.001732365906033928\n",
    "\n",
    "\n",
    "### debt_to_credit\n",
    "0.002165460944617781\n",
    "\n",
    "\n",
    "### prev_amt_to_credit\n",
    "0.000592535673183181\n",
    "\n",
    "\n",
    "### prev_down_payment_to_credit\n",
    "0.0005986699247162441\n",
    "\n",
    "\n",
    "### prev_credit_to_annuity\n",
    "0.0002223769402102116\n",
    "\n",
    "\n",
    "### total_payment\n",
    "0.0006312583891369683\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methods\n",
    "_(Describe the ML algorithms you used for questions [3.1](part-3.ipynb#Question-3.1), [4.1](part-3.ipynb#Question-4.1), and all others. Focus on the formulas, any feature extractions, parameter tuning, etc. Explain how the algorithm works. E.g., if you used a decision, don't say \"I used a decision tree\", explain briefly how a decision tree works and why it was ideally suited for the dataset you chose.)_ 3-5 paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdb42ea3806602e6aa3782bb2af3f2aa",
     "grade": true,
     "grade_id": "cell-616d5470b36b8952",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I have used stochastic gradient descent instead of a normal gradient descent algorithm since given the size of the dataset, stochastic gradient could give me a pretty good estimate after a short period of training phase, which helps me a lot with quick hyperparameter tuning and also estimating the model accuracy. \n",
    "\n",
    "Below are all the models that I have tested with the feature set provided above\n",
    "\n",
    "\n",
    "### Hinge Loss\n",
    "# $L(w,X) = \\frac{\\lambda}{2} \\left\\lVert w \\right\\rVert ^ 2 + \\frac{1}{n} \\sum_{i} \\max \\left[ 0, 1 - y_i \\widehat{y}_i\\right]$\n",
    "\n",
    "Hinge loss is training classifier primarily used in SVM, I have used this loss function simply wanted to do a comparison with another model to see the difference. As expected, given the same feature set this model performs the worst among all other models.\n",
    "\n",
    "### Linear Regression\n",
    "# $L(w,X) = \\frac{\\lambda}{2} \\left\\lVert w \\right\\rVert ^ 2 + \\frac{1}{n} \\sum_{i} \\frac{1}{2} \\left(f(w,x_i) - y_i\\right) ^ 2$\n",
    "\n",
    "Linear Regression is a features weighted algorithm. The loss function simply minimizes the square error between the predicted and the target label. However, the dataset given is not considered linear seperatable. Despite the fact that Linear Regression doess have some good performance it is still not comparable to logistic regression which could handle nonlinear relationships much better. \n",
    "\n",
    "### Logistic Regression\n",
    "<img src=\"logistic_loss.png\">\n",
    "\n",
    "Logisitc Regression is very similar to linear regression besides it takes log-odd of the ouput as loss function. Which turns out to be able to handle data that are not linearly seperatable much better than linear regression.\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Adding Regularization term to the model helps to prevent model overfitting by adding a penalty to the loss function. The regression model with regularization performs much better than the traditional model in this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "_(Provide some insights as to why you think that the proposed algorithms and features are good for this dataset. Explain whether you believe these are general properties that might be helpful for similar datasets--what makes them similar and why. What about this dataset made your solution successful. Could we use this for other datasets, if so, what types and why?)_ 3-5 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ae0fa2d0474dd5cc8c4555d77506c12",
     "grade": true,
     "grade_id": "cell-a5fad0534cafdb6f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### Features:\n",
    "\n",
    "I think some of the features discovered should be useful in all other similar dataset. There are some common financial measurements that could be extracted from the dataset which provides some intuition to all credit risk prediction problems.\n",
    "\n",
    "AMT_CREDIT:\n",
    "\n",
    "<img src=\"amt_credit_dist\">\n",
    "\n",
    "AMT_CREDIT is without a doubt a very significant feature in the dataset, a high credit amount of loan indicates higher difficulty for the client to pay back the loan and therefore lower probability of successful payment. \n",
    "\n",
    "\n",
    "AMT_INCOME_TOTAL:\n",
    "\n",
    "<img src=\"amt_income_total_dist\">\n",
    "\n",
    "AMT_INCOME_TOTAL is also very significant in this dataset. Yet we can clearly see that there is a outlier in the trainig set, I have remove it for better fitting. Higher income implies greater ability to pay back the loan.\n",
    "\n",
    "DEBT/CREDIT ratio:\n",
    "\n",
    "Debt to credit ratio has been popular in finance area for a long time. Lower ratio typically indicates the client has much stronger ability to pay back the loan since they have less financial problem.\n",
    "\n",
    "CREDIT/ANNUITY ratio:\n",
    "\n",
    "Credit to annuity ratio is also from the finance area, anuity well indicates the cash flow of the client, therefore lower ratio typically indicates better ability to pay back the loan.\n",
    "\n",
    "CREDIT-GOODS_PRICE:\n",
    "\n",
    "credit minus goods price indicates the amount of money left from the loan which is also extremely helpful with model prediction.\n",
    "\n",
    "DAYS_BIRTH:\n",
    "\n",
    "The age of the client has one of the most impact towards the clients' ability to pay back the loan. Not neccesseraily greater, but clients with age from 30-40 normally has the strongest financial ability to pay back the loan since the capital accumulation and responsibility from the family.\n",
    "\n",
    "OWN_CAR_AGE/DAYS_BIRTH:\n",
    "\n",
    "Own car age to days birth well indicates the portion of the clients life span that the clients has a greater ability to pay for life neccessarities. \n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "As discussed in related works, logistic regression has been one of the most popular model for banks to predict default rate for years. Despite the rise of neural net and deep learning this days, given the resources we have, running logistic regression could still give us a solid accuracy rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "_(Did you use all the data, cross-validation, training / test split, etc? Give enough details on how you setup the experiment so that your colleague can read this section and write their own algorithm to produce the same setup. Provide a link to the cells in the notebooks that contain the experimental setup.)_ 3-4 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7124b6b05c26327c785ea4baa791ee03",
     "grade": true,
     "grade_id": "cell-8e284da64965d4fd",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Data:\n",
    "\n",
    "I joint the following tables to extract desired data:\n",
    "application_train, bureau, previous_application, installments_payments\n",
    "\n",
    "All the tables are joined by sk_id_curr, and I took the average of feature value across all the rows with the same sk_id_curr. I think average value is the best indicator given the dataset since the number of rows with the same id in other tables could vary a lot.\n",
    "\n",
    "I used information gain, k-mean clustering, a lot of suggestions from the discussion forum, and some basic intuitions when it comes to feature selection. \n",
    "\n",
    "The model that I have tested was Hinge loss, classifier, linear regression and logistic regression. I have found with a 0.1 regularizer, 0.1 learning rate, 50 batch size, all the models seems to converged well. The logistic regression gives me the best outcome across all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "_(Write a table containing the results of your experiments, which were calculated in the notebooks. Include all algorithms included in questions [3.1](part-3.ipynb#Question-3.1) and above in Part 3 of the notebooks. Provide some interpretation of these results. Do you think you could have done better? If so, why did you not pursue those ideas? Add any pictures you think approprate here.)_ 5-6 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "47b0ae14537531217347e46db447bbba",
     "grade": true,
     "grade_id": "cell-384dadfd814ac391",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Model                       | Features                                                                        | Others Operations      | AUC    |   |\n",
    "|-----------------------------|---------------------------------------------------------------------------------|------------------------|--------|---|\n",
    "| Hinge Loss Classifier       | EXT_SOURCE_2, EXT_SOURCE_3, EXT_SOURCE_1, AVG(EXT_SOURCE), PRODUCT(EXT_SOOURCE) | Fill mean, z normalize | ~0.68  |   |\n",
    "| Linear Regression with L2   | EXT_SOURCE_2, EXT_SOURCE_3, EXT_SOURCE_1, AVG(EXT_SOURCE), PRODUCT(EXT_SOOURCE) | Fill mean, z normalize | ~0.69  |   |\n",
    "| Logistic Regression with L2 | EXT_SOURCE_2, EXT_SOURCE_3, EXT_SOURCE_1, AVG(EXT_SOURCE), PRODUCT(EXT_SOOURCE) | Fill mean, z normalize | 0.7095 |   |\n",
    "| Hinge Loss Classifier       | Final Feature Set | Fill mean, z normalize | ~0.71  |   |\n",
    "| Linear Regression with L2   | Final Feature Set | Fill mean, z normalize | ~0.71  |   |\n",
    "| Logistic Regression with L2 | Final Feature Set | Fill mean, z normalize | 0.722 |   |\n",
    "\n",
    "\n",
    "As the above table illustrates, The selected Feature and model set boosted the prediction accuracy by almost 0.04, which is a pretty good result. In the process I found fill mean and normalization also greatly affects the AUC. \n",
    "\n",
    "After reading more related papers and public kernels from kaggle, I found ensemble learning algorithms could also significantly improve the AUC, due the limited time, I did not have enough time to implement one of those, like Ada boost which mentioned in class. However, given extra time I would definitely implement and test it. I would suggest Ada boost could easily give the model an accuracy over 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "_(Summarize your findings. If someone wanted to use your solution, which would you recommend? What could you do if you had more data, etc. What should a company seeking to run this at high scale choose if they were to use your method.)_ 3 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfdbf631dd7ab4a6b983b5f57b05e7d8",
     "grade": true,
     "grade_id": "cell-fac0328f843eb6f5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "By doing this competition I found tons of interesting model that was not covered in class and some state of the art feature engineering. \n",
    "I found feature engineering in this problem has a huge affect to the model final AUC. \n",
    "\n",
    "Data cleaning, normalization, reducing noise/outliers also plays significant role. I spent most of my time on data analysis and feature engineering, I personally tested a lot of combinations of features to try to boost the information gain and AUC. \n",
    "\n",
    "Yet the time spent on discovering and implementing more algorithms was very limited. I would recommend anyone that wants to use my solution to try implement and test some other ensemble learning models first, since I found in a lot of studies and forums that it could boost the accuracy dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "_(Add a numbered list of the referenced articles, notebooks, etc. that you cited in the above notebooks. Pay attention that the numbers you used correspond to the list below.)_\n",
    "\n",
    "Consider the MLA or APA style, which should be available in Google Scholar.\n",
    "\n",
    "*Example*\n",
    "\n",
    "[[1]](https://arxiv.org) Bob Smith, John Doe. My amazing method. In _Proceedings of WWW 2018_, Lyon France, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97b044cabd4eb3042df6c03c3b625252",
     "grade": true,
     "grade_id": "cell-b608d1f5b8e2570a",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "[2](https://www.sciencedirect.com/science/article/pii/S1568494609001215) C.-F. Tsai, M.-L. Chen, \"Credit rating by hybrid machine learning techniques\", Applied soft computing, vol. 10, no. 2, pp. 374-380, 2010.\n",
    "\n",
    "[3](https://ieeexplore.ieee.org/document/7585216/references#references) R. E. Turkson, E. Y. Baagyere, G. E. Wenya, \"A machine learning approach for predicting bank credit worthiness\", 2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR), pp. 1-7, 2016.\n",
    "\n",
    "[3](https://www.sciencedirect.com/science/article/pii/S016792361400061X) SÃ©rgioMoro, PauloCortez, PauloRitaa, \"A data-driven approach to predict the success of bank telemarketing\", Decision Support Systems\n",
    "vol 62, pp. 22-31, 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix\n",
    "\n",
    "Add links to your part 1, part 2, and part 3 notebooks (using absolute links). \n",
    "Add anything else you want to here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "030e49a0552af8832a66591584da9636",
     "grade": true,
     "grade_id": "cell-a941b5ed7957cf69",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "part 3: http://notebooks.learnml.cool:31757/user/zhidongqu/notebooks/final-project/part-3.ipynb\n",
    "\n",
    "part 2:\n",
    "http://notebooks.learnml.cool:31757/user/zhidongqu/notebooks/final-project-2/part-2.ipynb\n",
    "\n",
    "part 1:\n",
    "http://notebooks.learnml.cool:31757/user/zhidongqu/notebooks/final-project-1/part-1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
